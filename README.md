# DBT-Airflow-Pipeline

This repository contains the code and resources for a **Data Engineering Pipeline** using **DBT and Apache Airflow**. The project involves setting up an **ELT pipeline** with Postgres, Docker, and DBT to transform and orchestrate data workflows.

---

## ğŸ“‚ Project Structure

```plaintext
.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ elt_dag.py               # Airflow DAG for ELT pipeline
â”‚   â”‚   â””â”€â”€ airflow.cfg              # Airflow configuration
â”‚   â”œâ”€â”€ custom_postgres/
â”‚   â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â”‚   â””â”€â”€ film_ratings_macro.sql  # SQL macros for DBT transformations
â”‚   â”‚   â”œâ”€â”€ models/example/
â”‚   â”‚   â”‚   â”œâ”€â”€ actors.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ film_actors.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ film_ratings.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ films.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ specific_movie.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ schema.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.yml
â”‚   â”‚   â”‚   â””â”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ elt/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ elt_script.py             # Python script to load data from source to destination
â”‚   â”‚   â”œâ”€â”€ start.sh                  # Shell script to initialize services
â”‚   â”œâ”€â”€ source_db_init/
â”‚   â”‚   â”œâ”€â”€ init.sql                  # SQL script to initialize source database
â”‚   â”‚   â”œâ”€â”€ docker-compose.yaml       # Docker Compose file for setting up the database
â”‚   â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md                          # Project documentation
```

---

## ğŸš€ Project Workflow

### 1. **Data Pipeline Setup**
- Used **Docker** and **Docker Compose** to containerize the database and orchestrate services.
- Configured **Postgres** with two databases (**source_db** and **destination_db**).
- Used **bridge networking** to enable communication between containers.

### 2. **ELT Process**
- Created **`elt_script.py`** using `subprocess` and `time` libraries to transfer data from `init.sql`.
- Defined **Airflow DAGs** (`elt_dag.py`) to automate data extraction and loading.
- Scheduled a **CRON job** for automated execution.

### 3. **DBT Transformation**
- Configured **DBT profiles** and `dbt_project.yml`.
- Created **SQL models** to transform data efficiently.
- Utilized **macros** for reusable SQL components.
- Defined **schemas and sources** (`schema.yml`, `sources.yml`) for data consistency.

### 4. **Orchestration with Airflow**
- Initialized **Airflow web server** to monitor and manage DAGs.
- Orchestrated **ELT and DBT tasks** using Airflow.
- Implemented **task dependencies** for smooth execution.

---

## âš™ï¸ How to Run the Project

1. Clone this repository:
   ```bash
   git clone https://github.com/jha-aman09/DBT-Airflow-Pipeline.git
   ```
2. Navigate to the project directory:
   ```bash
   cd DBT-Airflow-Pipeline
   ```
3. Start the Docker containers:
   ```bash
   docker-compose up -d
   ```

---

## ğŸ“Š Key Features

- **Containerized Environment**: Used Docker and Docker Compose.
- **Automated ELT Pipeline**: Airflow DAGs handle extraction and loading.
- **DBT for Transformations**: Modular SQL models and macros for efficiency.
- **Airflow Orchestration**: Web UI for scheduling and monitoring workflows.
- **Scalable Architecture**: Easily extendable for new data sources and transformations.

---

## ğŸ› ï¸ Potential Improvements

- Integrate **CI/CD pipelines** for deployment automation.
- Implement **logging and monitoring** for error tracking.
- Optimize **DBT models** for faster transformation.
- Extend pipeline to **cloud-based storage (AWS/GCP/Azure)**.

---

## ğŸ§‘â€ğŸ’» Author  
- **LinkedIn**: [Aman Jha](https://www.linkedin.com/in/aman--jha/)  
- **GitHub**: [Aman Jha](https://github.com/jha-aman09)
